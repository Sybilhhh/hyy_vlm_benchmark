# VLM Benchmark Configuration Example
models:
  gpt-4o:
    type: api
    model_name: "gpt-4o"
    endpoint: "https://openaieastus2instance.openai.azure.com/openai/deployments/gpt-4o-cv-chx0812/chat/completions?api-version=2025-01-01-preview"
    api_version: "2024-12-01-preview"
    api_key: "990a353da7b44bef8466402378c486cd"
    max_tokens: 4096
    temperature: 0
    top_p: 1.0
  
  qwen2.5vl-7b:
    type: local
    model_path: "/data/models/Qwen2.5-VL-7B-Instruct"
    device: "auto"
    max_pixels: 4608
    fps: 1
    max_new_tokens: 512

  tarsier2-7b:
    type: local
    model_path: "/data/models/Tarsier2-7b-0115"
    device: "auto"
    max_new_tokens: 512
    max_n_frames: 256
    n_frames: 16
    max_pixels: 460800 # 1280 * 720 // 2
    min_pixels: 0
    max_seq_len: 16384
    is_training: false  # 会影响：1. 训练和测试时采帧不同；2. 测试时忽略 response。
    print_data_error: true
    is_training: false
    do_image_padding: false
    do_image_crop: false
    do_image_resize: false
    video_sampling_strategy: {'video_sampler_version': 'v1', 'force_frames_n_divisible': 1, 'use_multi_images_for_video': true}
    prompt: ""
    train_task: sft
    temperature: 0

datasets:
  dream-1k:
    data_path: "/media/sda/vlm_datasets/DREAM-1K"
    annotation_file: "/media/sda/vlm_datasets/DREAM-1K/json/metadata.json"
    max_samples: 0
    load_predictions_from_file: "/workspace/videocaptionevaluation/output/test_etva/predictions/predictions.json"

  et-bench-captioning:
    data_path: "/data/datasets/ETBench"
    annotation_file: "/data/datasets/ETBench/annotations/etbench_txt_dvc_slc_test.json"
    max_samples: 0
    load_predictions_from_file: null


evaluators:
  autodq:
    type: autodq
    model_name: "gpt-4o"
    endpoint: "https://openaieastus2instance.openai.azure.com/openai/deployments/gpt-4o-cv-chx0812/chat/completions?api-version=2025-01-01-preview"
    api_version: "2024-12-01-preview"
    api_key: "990a353da7b44bef8466402378c486cd"
    max_tokens: 4096
    temperature: 1.0
    top_p: 1.0

  etbench:
    type: etbench
    sentence_model: "/data/datasets/ETBench/all-MiniLM-L6-v2"

  etva:
    type: etva
    llm:
      model_name: "gpt-4o"
      endpoint: "https://openaieastus2instance.openai.azure.com/openai/deployments/gpt-4o-cv-chx0812/chat/completions?api-version=2025-01-01-preview"
      api_version: "2024-12-01-preview"
      api_key: "990a353da7b44bef8466402378c486cd"
      max_tokens: 150
      temperature: 0
      top_p: 0.95
      repetition_penalty: 1.2
      stop: ["Example","Input","Task","Entity"]
    vlm-qwen:
      type: local
      model_path: "/data/models/Qwen2.5-VL-7B-Instruct"
      device: "auto"
      max_pixels: 460800
      fps: 1
      max_new_tokens: 64
    vlm-tarsier:
      type: local
      model_path: "/data/models/Tarsier2-7b-0115"
      device: "auto"
      max_new_tokens: 512
      max_n_frames: 256
      n_frames: 16
      max_pixels: 460800 # 1280 * 720 // 2
      min_pixels: 0
      max_seq_len: 16384
      is_training: false  # 会影响：1. 训练和测试时采帧不同；2. 测试时忽略 response。
      print_data_error: true
      is_training: false
      do_image_padding: false
      do_image_crop: false
      do_image_resize: false
      video_sampling_strategy: {'video_sampler_version': 'v1', 'force_frames_n_divisible': 1, 'use_multi_images_for_video': true}
      prompt: ""
      train_task: sft
      temperature: 0



logging:
  level: INFO
  file: "./logs/benchmark.log"
